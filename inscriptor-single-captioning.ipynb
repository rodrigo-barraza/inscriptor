{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKASCQg9nU5"
      },
      "source": [
        "# Rodrigo Barraza's Inscriptions: Blip 2 Mass Captioning\n",
        "Large RAM and VRAM is required to load the larger models. RAM should be at least 24-32GB with 64GB being optimal. VRAM should be at least 16GB or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mLI_ic0j9nU-",
        "outputId": "4b079d97-cddd-41ec-8936-23518f8a129a"
      },
      "outputs": [],
      "source": [
        "# !pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration, AutoTokenizer, Blip2Model\n",
        "import torch\n",
        "import sys\n",
        "import validators\n",
        "# from lavis.models import load_model_and_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Salesforce/blip2-opt-2.7b\n",
        "# Salesforce/blip2-opt-2.7b-coco\n",
        "# Salesforce/blip2-opt-6.7b\n",
        "# Salesforce/blip2-opt-6.7b-coco\n",
        "# Salesforce/blip2-flan-t5-xl\n",
        "# Salesforce/blip2-flan-t5-xl-coco\n",
        "# /Salesforce/blip2-flan-t5-xxl\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\")\n",
        "    # add_prefix_space=True) # Required to use bad_words_ids\n",
        "processor = Blip2Processor.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\",\n",
        "    device_map='auto',\n",
        "    # load_in_8bit=True)\n",
        "    torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# Start of Options\n",
        "# imagesDirectory = \"/mnt/d/dataset-1080\"\n",
        "# useFolderNamesAsTokens = True  # Append the folder names to the beginning of the caption\n",
        "# overwriteExistingCaptions = False  # Overwrite existing captions\n",
        "# tokensStartOrEnd = 'start'  # end or start\n",
        "# useNucleusSampling = False\n",
        "# appendStyles = True\n",
        "# showImages = False\n",
        "image_url_or_path = \"/mnt/d/fixes/0033836.jpg\"\n",
        "\n",
        "# PREPROCESSOR SETTINGS\n",
        "skipSpecialTokens = True\n",
        "\n",
        "# MODEL GENERATION SETTINGS\n",
        "useNucleusSampling = False\n",
        "numberOfBeams = 1  # The number of beams to use for beam search\n",
        "minTokenLength = 15  # The amount of minimum tokens to generate\n",
        "maxTokenLength = 15  # The maximum amount of tokens to generate\n",
        "repetitionPenalty = 1.0\n",
        "lengthPenalty = 1.0\n",
        "topP = 0.9\n",
        "temperature = 1.0\n",
        "\n",
        "# TOKENIZER SETTINGS\n",
        "enableForceWords = False\n",
        "forceWordsList = [\"a kid named turtle\"]\n",
        "removeBadWords = False\n",
        "badWordsList = [\"teddy bear\"]\n",
        "padding = True\n",
        "add_special_tokens = False\n",
        "\n",
        "\n",
        "# prompt = \"Describe the style in 1 word. Answer:\"\n",
        "prompt = None\n",
        "# End of Options\n",
        "###############################################################################\n",
        "\n",
        "forceWordsIds = None\n",
        "badWordsIds = None\n",
        "\n",
        "if (validators.url(image_url_or_path)):\n",
        "    image = Image.open(\n",
        "        requests.get(\n",
        "        image_url_or_path,\n",
        "        stream=True).raw).convert('RGB')\n",
        "else:\n",
        "    image = Image.open(image_url_or_path).convert('RGB')\n",
        "\n",
        "if enableForceWords:\n",
        "    forceWords = tokenizer(\n",
        "        forceWordsList,\n",
        "        padding=padding,\n",
        "        add_special_tokens=add_special_tokens,\n",
        "        return_tensors=\"pt\").to(device).input_ids\n",
        "    forceWordsIds = forceWords.tolist()  # Convert the tensor to a list of lists\n",
        "\n",
        "if removeBadWords:\n",
        "    badWords = tokenizer(\n",
        "        badWordsList,\n",
        "        padding=padding,\n",
        "        add_special_tokens=add_special_tokens,\n",
        "        return_tensors=\"pt\").to(device).input_ids\n",
        "    badWordsIds = badWords.tolist()  # Convert the tensor to a list of lists\n",
        "\n",
        "# bad_words = tokenizer([\"teddy bear\"], padding=True, add_special_tokens=False, return_tensors=\"pt\").to(device).input_ids\n",
        "# bad_words_list = bad_words.tolist()  # Convert the tensor to a list of lists\n",
        "\n",
        "inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **inputs,\n",
        "    do_sample=useNucleusSampling,\n",
        "    force_words_ids=forceWordsIds,\n",
        "    bad_words_ids=badWordsIds,\n",
        "#    bad_words_ids=bad_words_list,\n",
        "    num_beams=numberOfBeams,\n",
        "    max_length=maxTokenLength,\n",
        "    min_length=minTokenLength,\n",
        "    repetition_penalty=repetitionPenalty,\n",
        "    length_penalty=lengthPenalty,\n",
        "    top_p=topP,\n",
        "    # num_return_sequences=5,\n",
        "    temperature=temperature)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)[0].strip()\n",
        "display(image.resize(( int(image.width * 0.333), int(image.height * 0.333))))\n",
        "print(\"Caption: \", generated_text)\n",
        "\n",
        "if prompt:\n",
        "    inputs = processor(images=image, text=\"Describe the style in 1 word. Answer:\", return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_length=10,\n",
        "        min_length=7,\n",
        "        do_sample=useNucleusSampling,\n",
        "        force_words_ids=forceWordsIds,\n",
        "        bad_words_ids=badWordsIds,\n",
        "    #    bad_words_ids=bad_words_list,\n",
        "        num_beams=numberOfBeams,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        temperature=temperature)\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "    print(prompt)\n",
        "    print(generated_text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Captioner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# url = \"https://huggingface.co/spaces/Salesforce/BLIP2/resolve/main/house.png\"\n",
        "# image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
        "\n",
        "# prompt = \"Question: How could someone get out of the house? Answer:\"\n",
        "# inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "# generated_ids = model.generate(**inputs)\n",
        "# generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "# print(generated_text)\n",
        "\n",
        "\n",
        "import validators\n",
        "\n",
        "url = '/mnt/d/dataset-1080/_/0014596.jpg'\n",
        "\n",
        "if (validators.url(url)):\n",
        "    image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
        "else:\n",
        "    image = Image.open(url).convert('RGB')  \n",
        "\n",
        "display(image.resize((596, 437)))\n",
        "inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "# Greedy Decoding (default)\n",
        "print(\"###Greedy###\")\n",
        "generated_ids = model.generate(**inputs)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "print(generated_text)\n",
        "# Beam Search Decoding\n",
        "print(\"###Beam Search###\")\n",
        "generated_ids = model.generate(**inputs,\n",
        "                               num_beams=5,\n",
        "                               max_length=20,\n",
        "                               min_length=15,\n",
        "                               repetition_penalty=1.0,\n",
        "                               length_penalty=1.0,\n",
        "                               top_p=0.9,\n",
        "                               temperature=1)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "print(generated_text)\n",
        "# Nucleus Sampling Decoding\n",
        "print(\"###Nucleus Sampling###\")\n",
        "generated_ids= model.generate(**inputs, do_sample=True, top_p=0.9)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "print(generated_text)\n",
        "\n",
        "t = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "force_words = t(\"planet\", add_prefix_space=True, add_special_tokens=False).input_ids\n",
        "\n",
        "\n",
        "# image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
        "# model.generate({\"image\": image}, use_nucleus_sampling=False, num_captions=1, min_length=15, max_length=20)\n",
        "\n",
        "\n",
        "\n",
        "# prompt = \"Question: Describe the style in 1 word? Answer:\"\n",
        "# inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "# generated_ids = model.generate(**inputs, num_beams=2)\n",
        "# generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "# print(generated_text)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "85AsbJrs9nVF"
      },
      "source": [
        "#### Load BLIP2 captioning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "CAObkxLK9nVF",
        "outputId": "59ab7fa9-2410-4112-8a12-da164d742ae1"
      },
      "outputs": [],
      "source": [
        "# setup device to use\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "# we associate a model with its preprocessors to make it easier for inference.\n",
        "model, vis_processors, _ = load_model_and_preprocess(\n",
        "    # name=\"blip2_opt\", model_type=\"pretrain_opt2.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_opt\", model_type=\"pretrain_opt6.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_opt\", model_type=\"caption_coco_opt2.7b\", is_eval=True, device=device\n",
        "    name=\"blip2_opt\", model_type=\"caption_coco_opt6.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_t5\", model_type=\"pretrain_flant5xl\", is_eval=True, device=device\n",
        "    # name=\"blip2_t5\", model_type=\"caption_coco_flant5xl\", is_eval=True, device=device\n",
        "    # This next model is one scary devil in terms of size...\n",
        "    # ... it requires at least 32GB of VRAM to run...\n",
        "    # ... and will not load on 3090s or 4090s.\n",
        "    # name=\"blip2_t5\", model_type=\"pretrain_flant5xxl\", is_eval=True, device=device\n",
        ")\n",
        "\n",
        "vis_processors.keys()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wfO5NdZv9nVH"
      },
      "source": [
        "#### Auto Caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "###############################################################################\n",
        "# Start of Options\n",
        "imagesDirectory = \"/mnt/d/dataset-1080\"\n",
        "useFolderNamesAsTokens = True  # Append the folder names to the beginning of the caption\n",
        "overwriteExistingCaptions = False  # Overwrite existing captions\n",
        "tokensStartOrEnd = 'start'  # end or start\n",
        "minTokenLength = 15  # The amount of minimum tokens to generate\n",
        "maxTokenLength = 20  # The maximum amount of tokens to generate\n",
        "\n",
        "useNucleusSampling = False\n",
        "repetitionPenalty = 1\n",
        "\n",
        "appendStyles = True\n",
        "\n",
        "showImages = False\n",
        "# End of Options\n",
        "###############################################################################\n",
        "\n",
        "numberOfCaptions = 1  # How many captions to generate\n",
        "# Count the total number of images in the directory and subdirectories\n",
        "totalImages = 0\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    totalImages += sum([filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\")) for filename in filenames])\n",
        "\n",
        "processedImages = 0\n",
        "\n",
        "\n",
        "def process_images(dirpath):\n",
        "    global processedImages\n",
        "    suspects = os.listdir(dirpath)\n",
        "    imageSuspects = [filename for filename in suspects if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\"))]\n",
        "\n",
        "    # Process each image\n",
        "    for suspectIndex in range(len(imageSuspects)):\n",
        "        processedImages += 1\n",
        "        remainingImages = totalImages - processedImages\n",
        "        caption = \"\"\n",
        "        print(f\"Processed images: {processedImages}/{totalImages}\")\n",
        "        print(f\"Remaining images: {remainingImages}\")\n",
        "\n",
        "        # Load Image\n",
        "        imagePath = imageSuspects[suspectIndex]\n",
        "        imageFilePath = dirpath + \"/\" + imagePath\n",
        "        textFilePath = Path(imageFilePath).with_suffix('.txt')\n",
        "\n",
        "        # If the image hasn't already been processed, caption it\n",
        "        if overwriteExistingCaptions or not os.path.exists(textFilePath):\n",
        "            rawImage = Image.open(imageFilePath).convert('RGB')\n",
        "            # Display the image as it's been processed\n",
        "            if showImages:\n",
        "                display(rawImage)\n",
        "            image = vis_processors[\"eval\"](rawImage).unsqueeze(0).to(device)\n",
        "            imageCaption = model.generate({\"image\": image}, min_length=minTokenLength, max_length=maxTokenLength, use_nucleus_sampling=useNucleusSampling, num_captions=numberOfCaptions, repetition_penalty=repetitionPenalty)\n",
        "\n",
        "            modifiedCaption = imageCaption[0]\n",
        "            captionWords = modifiedCaption.split()\n",
        "\n",
        "            # Fix grammatical spelling errors by BLIP2\n",
        "            if \"laying\" in captionWords:\n",
        "                modifiedCaption = modifiedCaption.replace('laying', 'lying')\n",
        "\n",
        "            # Append/Prepend folder names to the caption\n",
        "            relpath = os.path.relpath(dirpath, imagesDirectory)\n",
        "            relpathParts = [part for part in relpath.split(os.sep) if \"_\" not in part and part != \".\"]\n",
        "            validParts = [part for part in relpathParts if part.lower() not in captionWords and part.lower() not in modifiedCaption]\n",
        "            if useFolderNamesAsTokens and validParts:\n",
        "                if tokensStartOrEnd == 'end':\n",
        "                    caption = f\"{modifiedCaption}, {', '.join(validParts)}\"\n",
        "                else:\n",
        "                    caption = f\"{', '.join(validParts)}, {modifiedCaption}\"\n",
        "\n",
        "            else:\n",
        "                caption = imageCaption[0]\n",
        "\n",
        "            # Append answers to the caption\n",
        "            if appendStyles:\n",
        "                style = model.generate({\"image\": image, \"prompt\": \"Describe the style in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=1, min_length=5, max_length=15)\n",
        "                theme = model.generate({\"image\": image, \"prompt\": \"Describe the theme in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=1, min_length=5, max_length=15)\n",
        "                background = model.generate({\"image\": image, \"prompt\": \"Describe object in the background in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=1, min_length=5, max_length=15)\n",
        "                medium = model.generate({\"image\": image, \"prompt\": \"Describe the medium in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=1, min_length=5, max_length=15)\n",
        "                color = model.generate({\"image\": image, \"prompt\": \"Describe the color in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=1, min_length=5, max_length=15)\n",
        "                person = model.generate({\"image\": image, \"prompt\": \"Describe the person in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=1, min_length=5, max_length=15)\n",
        "                outfit = model.generate({\"image\": image, \"prompt\": \"Describe the outfit in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=1, min_length=5, max_length=15)\n",
        "                \n",
        "                combined = OrderedDict()\n",
        "                combinedLists = person + outfit + background + style + theme + medium + color\n",
        "                answers = [item.strip() for sublist in combinedLists for item in re.split(',|and', sublist) if item.strip()]\n",
        "\n",
        "                for answer in answers:\n",
        "                    lowerWord = answer.lower().lstrip()\n",
        "\n",
        "                    if '_' in lowerWord or '??' in lowerWord or '!!' in lowerWord:\n",
        "                        lowerWord = ''\n",
        "\n",
        "                    if 'f**k' in lowerWord:\n",
        "                        lowerWord.replace('f**k', 'fuck')\n",
        "\n",
        "                    if lowerWord.startswith(('a ', 'the ', 'and ')):\n",
        "                        lowerWord = lowerWord.split(' ', 1)[1]\n",
        "\n",
        "                    if lowerWord.endswith(('.', ',', '!', '?')):\n",
        "                        lowerWord = lowerWord[:-1]\n",
        "\n",
        "                    if len(lowerWord) > 1:\n",
        "                        combined[lowerWord] = None\n",
        "\n",
        "                uniqueCombinedArray = list(combined)\n",
        "\n",
        "                uniqueImageAnswers = set(answer.lower() for answer in uniqueCombinedArray)\n",
        "                filteredImageAnswers = [ans for ans in uniqueImageAnswers if not re.search(rf'\\b{re.escape(ans)}\\b', caption.lower())]\n",
        "\n",
        "                if filteredImageAnswers:\n",
        "                    caption += ', ' + ', '.join(filteredImageAnswers)\n",
        "\n",
        "            # Remove periods\n",
        "            caption.replace('.', '')\n",
        "\n",
        "            # Save Caption as .txt file\n",
        "            with open(textFilePath, 'w+') as f:\n",
        "                f.write(caption)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        print(caption)\n",
        "        print(imageFilePath)\n",
        "                     \n",
        "# Iterate through directories inside directories\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    process_images(dirpath)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "blip2demo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "50c50f1f55c832fff615d4f17bdf1949c2ce06a8f6fb0f097854f91355ce9518"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
