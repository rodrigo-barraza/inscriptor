{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKASCQg9nU5"
      },
      "source": [
        "# Rodrigo Barraza's Inscriptions: Blip 2 Mass Captioning\n",
        "Large RAM and VRAM is required to load the larger models. RAM should be at least 24-32GB with 64GB being optimal. VRAM should be at least 16GB or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mLI_ic0j9nU-",
        "outputId": "4b079d97-cddd-41ec-8936-23518f8a129a"
      },
      "outputs": [],
      "source": [
        "# !pip3 install transformers\n",
        "# !pip install pillow\n",
        "# !pip install requests\n",
        "# !pip install validators\n",
        "# !pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration, AutoTokenizer, Blip2Model\n",
        "import torch\n",
        "import sys\n",
        "import validators\n",
        "\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# from lavis.models import load_model_and_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Salesforce/blip2-opt-2.7b\n",
        "# Salesforce/blip2-opt-2.7b-coco\n",
        "# Salesforce/blip2-opt-6.7b\n",
        "# Salesforce/blip2-opt-6.7b-coco\n",
        "# Salesforce/blip2-flan-t5-xl\n",
        "# Salesforce/blip2-flan-t5-xl-coco\n",
        "# /Salesforce/blip2-flan-t5-xxl\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\")\n",
        "    # add_prefix_space=True) # Required to use bad_words_ids\n",
        "processor = Blip2Processor.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\",\n",
        "    device_map='auto',\n",
        "    # load_in_8bit=True)\n",
        "    torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleanedUpCaption = \"the smart dog dog dog walked down the street\"\n",
        "pattern = re.compile(r'\\b(\\w+)\\b\\s+\\b\\1\\b')\n",
        "match = pattern.search(cleanedUpCaption.lower())\n",
        "if match:\n",
        "    start = cleanedUpCaption.lower().find(match.group(1))\n",
        "    cleanedUpCaption = cleanedUpCaption[:start] + match.group(1)\n",
        "print(cleanedUpCaption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "imagesDirectory = \"/mnt/d/dataset-1080-new\"\n",
        "\n",
        "# Get all images that do not have a corresponding text file\n",
        "datasetDirectoryName = os.path.basename(imagesDirectory)\n",
        "textFilePath = f\"{imagesDirectory}/{datasetDirectoryName}.txt\"\n",
        "image_paths = []\n",
        "image_extensions = (\".jpg\", \".png\", \".jpeg\", \".webp\", \".gif\")\n",
        "for root, dirs, files in os.walk(imagesDirectory):\n",
        "    # Filter out image files and text files\n",
        "    image_files = [f for f in files if f.lower().endswith(image_extensions)]\n",
        "    txt_files = {f[:-4] for f in files if f.lower().endswith(\".txt\")}\n",
        "\n",
        "    for f in image_files:\n",
        "        # Check if the image file does not have a corresponding txt file\n",
        "        if f[:-4] not in txt_files:\n",
        "            image_paths.append(os.path.join(root, f))\n",
        "\n",
        "separator = '\\n'\n",
        "joined_string = separator.join(image_paths)\n",
        "\n",
        "with open(textFilePath, 'w+') as f:\n",
        "    f.write(joined_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# Start of Options\n",
        "\n",
        "imagesDirectory = \"/mnt/d/dataset-1080-new\"\n",
        "useFolderNamesAsTokens = True  # Append the folder names to the beginning of the caption\n",
        "overwriteExistingCaptions = False  # Overwrite existing captions\n",
        "tokensStartOrEnd = 'start'  # end or start\n",
        "appendStyles = True\n",
        "\n",
        "# PRINT OPTIONS\n",
        "showCurrentImage = False\n",
        "showCaption = True\n",
        "showRecaptionedImages = True\n",
        "showProcessedImages = True\n",
        "showRemainingImages = True\n",
        "clearOutput = True\n",
        "\n",
        "# PREPROCESSOR SETTINGS\n",
        "skipSpecialTokens = True\n",
        "\n",
        "# MODEL GENERATION SETTINGS\n",
        "useNucleusSampling = False\n",
        "numberOfBeams = 3  # The number of beams to use for beam search\n",
        "lengthPenalty = 1\n",
        "minTokenLength = 5  # The amount of minimum tokens to generate\n",
        "maxTokenLength = 72  # The maximum amount of tokens to generate\n",
        "repetitionPenalty = 1\n",
        "topP = 0.9\n",
        "temperature = 1.0\n",
        "\n",
        "# TOKENIZER SETTINGS\n",
        "# enableForceWords = False\n",
        "# forceWordsList = [\"water\"]\n",
        "# removeBadWords = False\n",
        "# badWordsList = [\"teddy bear\"]\n",
        "# padding = True\n",
        "# add_special_tokens = False\n",
        "\n",
        "\n",
        "# prompt = \"Describe the style in 1 word. Answer:\"\n",
        "prompt = None\n",
        "\n",
        "# End of Options\n",
        "###############################################################################\n",
        "\n",
        "forceWordsIds = None\n",
        "badWordsIds = None\n",
        "primaryCaptionTokenLength = 0\n",
        "\n",
        "# Count the total number of images in the directory and subdirectories\n",
        "totalImages = 0\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    totalImages += sum([filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\")) for filename in filenames])\n",
        "processedImages = 0\n",
        "recaptionedImages = 0\n",
        "\n",
        "def generateCaption(rawImage, beams=numberOfBeams):\n",
        "    global forceWordsIds\n",
        "    global badWordsIds\n",
        "    generatedCaption = ''\n",
        "\n",
        "    # if enableForceWords:\n",
        "    #     forceWords = tokenizer(\n",
        "    #         forceWordsList,\n",
        "    #         padding=padding,\n",
        "    #         add_special_tokens=add_special_tokens,\n",
        "    #         return_tensors=\"pt\").to(device).input_ids\n",
        "    #     forceWordsIds = forceWords.tolist()  # Convert the tensor to a list of lists\n",
        "\n",
        "    # if removeBadWords:\n",
        "    #     badWords = tokenizer(\n",
        "    #         badWordsList,\n",
        "    #         padding=padding,\n",
        "    #         add_special_tokens=add_special_tokens,\n",
        "    #         return_tensors=\"pt\").to(device).input_ids\n",
        "    #     badWordsIds = badWords.tolist()  # Convert the tensor to a list of lists\n",
        "\n",
        "    inputs = processor(images=rawImage, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=useNucleusSampling,\n",
        "        force_words_ids=forceWordsIds,\n",
        "        bad_words_ids=badWordsIds,\n",
        "        num_beams=beams,\n",
        "        max_length=maxTokenLength,\n",
        "        min_length=minTokenLength,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    primaryCaptionTokenLength = torch.numel(generated_ids)\n",
        "    # If token length is too long (because it's repeated words over and over), try again with different settings\n",
        "    if primaryCaptionTokenLength >= 72:\n",
        "        if showRemainingImages:\n",
        "            print(\"Recaptioning... ⚠️\")\n",
        "        generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=useNucleusSampling,\n",
        "        force_words_ids=forceWordsIds,\n",
        "        bad_words_ids=badWordsIds,\n",
        "        num_beams=10,\n",
        "        max_length=maxTokenLength,\n",
        "        min_length=minTokenLength,\n",
        "        repetition_penalty=1.5,\n",
        "        length_penalty=-1,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    generatedCaption = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "    generatedCaption = generatedCaption[0].strip()\n",
        "    # If token contains repeated words around ' of a ' or ' of an ', try again with different settings\n",
        "    regexPatterns = [\n",
        "        r'\\b(\\w+)\\b of a \\b\\1\\b',\n",
        "        r'\\b(\\w+)\\b of an \\b\\1\\b'\n",
        "    ]\n",
        "    if any(re.search(pattern, generatedCaption) for pattern in regexPatterns):\n",
        "        generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=useNucleusSampling,\n",
        "        num_beams=9,\n",
        "        max_length=maxTokenLength,\n",
        "        min_length=minTokenLength,\n",
        "        repetition_penalty=1.9,\n",
        "        length_penalty=1,\n",
        "        top_p=topP,\n",
        "        temperature=1)\n",
        "        generatedCaption = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "        generatedCaption = generatedCaption[0].strip()\n",
        "        for pattern in regexPatterns:\n",
        "            if re.search(pattern, generatedCaption):\n",
        "                generatedCaption = re.sub(pattern, lambda match: ' '.join(match.group().split()[:-3]), generatedCaption)\n",
        "    return cleanUpCaption(generatedCaption)\n",
        "\n",
        "def generateFolderNamesAsTokens(caption):\n",
        "    generatedCaption = caption\n",
        "    relpath = os.path.relpath(dirpath, imagesDirectory)\n",
        "    relpathParts = [part for part in relpath.split(os.sep) if \"_\" not in part and part != \".\"]\n",
        "    validParts = [part for part in relpathParts if part.lower() not in caption and part.lower()]\n",
        "    if validParts:\n",
        "        if tokensStartOrEnd == 'end':\n",
        "            generatedCaption = f\"{caption}, {', '.join(validParts)}\"\n",
        "        else:\n",
        "            generatedCaption = f\"{', '.join(validParts)}, {caption}\"\n",
        "    return generatedCaption\n",
        "\n",
        "def generateExtraDescriptors(rawImage, caption):\n",
        "    generatedExtraDescriptions = ''\n",
        "    inputs = processor(images=rawImage, text=\"Describe the style in 1 word. Answer:\", return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        max_length=10,\n",
        "        min_length=1,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    style = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "    inputs = processor(images=rawImage, text=\"Describe the theme in 1 word. Answer:\", return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        max_length=10,\n",
        "        min_length=1,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    theme = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "    inputs = processor(images=rawImage, text=\"Describe the medium in 1 word. Answer:\", return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        max_length=10,\n",
        "        min_length=1,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    medium = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "\n",
        "    combined = OrderedDict()\n",
        "    combinedLists = style + theme + medium\n",
        "    answers = [item.strip() for sublist in combinedLists for item in re.split(',|and', sublist) if item.strip()]\n",
        "\n",
        "    for answer in answers:\n",
        "        lowerWord = answer.lower().lstrip()\n",
        "\n",
        "        if '_' in lowerWord or '??' in lowerWord or '!!' in lowerWord or '—' in lowerWord or '~' in lowerWord or '@' in lowerWord or '|' in lowerWord:\n",
        "            lowerWord = ''\n",
        "\n",
        "        if lowerWord.startswith(('a ', 'the ', 'and ')):\n",
        "            lowerWord = lowerWord.split(' ', 1)[1]\n",
        "\n",
        "        if lowerWord.endswith(('.', ',', '!', '?')):\n",
        "            lowerWord = lowerWord[:-1]\n",
        "\n",
        "        if lowerWord.endswith(\"'\") and lowerWord.startswith(\"'\"):\n",
        "            lowerWord = lowerWord[:-1]\n",
        "            lowerWord = lowerWord[1:]\n",
        "\n",
        "        if len(lowerWord) > 1:\n",
        "            combined[lowerWord] = None\n",
        "\n",
        "    uniqueCombinedArray = list(combined)\n",
        "    uniqueImageAnswers = set(answer.lower() for answer in uniqueCombinedArray)\n",
        "    filteredImageAnswers = [ans for ans in uniqueImageAnswers if not re.search(rf'\\b{re.escape(ans)}\\b', caption.lower())]\n",
        "    if filteredImageAnswers:\n",
        "        generatedExtraDescriptions = caption + ', ' + ', '.join(filteredImageAnswers)\n",
        "    else:\n",
        "        generatedExtraDescriptions = caption\n",
        "    generatedExtraDescriptions = cleanUpCaption(generatedExtraDescriptions)\n",
        "    return generatedExtraDescriptions\n",
        "\n",
        "def cleanUpCaption(caption):\n",
        "    cleanedUpCaption = caption\n",
        "\n",
        "    if cleanedUpCaption.endswith((\"'s\")):\n",
        "        cleanedUpCaption = cleanedUpCaption[:-2]\n",
        "        \n",
        "    if cleanedUpCaption.endswith((\".\")):\n",
        "        cleanedUpCaption = cleanedUpCaption[:-1]\n",
        "\n",
        "    if \" - \" in cleanedUpCaption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"t - shirt\", \"t-shirt\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\" - man\", \"-man\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\" - men\", \"-men\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"t - rex\", \"t-rex\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"sci - fi\", \"sci-fi\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"x - files\", \"x-files\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\" - stock image\", \"\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"bb - 8\", \"bb-8\")\n",
        "        \n",
        "    \n",
        "    cleanedUpCaption = cleanedUpCaption.replace('pokémon', 'pokemon')\n",
        "    cleanedUpCaption = cleanedUpCaption.replace(\"http\", \"\").replace(\"www\", \"\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace(\"/\", \"\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace('\"', \"\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace(\"t.v.\", \"television\").replace(\"t.v\", \"television\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace(\" & \", \"&\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace(\"black & white\", \"black and white\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace('laying', 'lying')\n",
        "\n",
        "    if cleanedUpCaption.count(' - ') >= 3:\n",
        "        split_text = cleanedUpCaption.split('-', 1)\n",
        "        cleanedUpCaption = split_text[0]\n",
        "\n",
        "    if \"blanka\" in cleanedUpCaption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"hulk\", \"blanka\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"green hair\", \"orange hair\")\n",
        "    return cleanedUpCaption.strip()\n",
        "\n",
        "def captionImages(dirpath):\n",
        "    global processedImages\n",
        "    global recaptionedImages\n",
        "    suspects = os.listdir(dirpath)\n",
        "\n",
        "    imageSuspects = [filename for filename in suspects if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\"))]\n",
        "\n",
        "    # Get all images that do not have a corresponding text file\n",
        "    # datasetDirectoryName = os.path.basename(imagesDirectory)\n",
        "    # textFilePath = f\"{imagesDirectory}/{datasetDirectoryName}.txt\"\n",
        "    # image_paths = []\n",
        "    # image_extensions = (\".jpg\", \".png\", \".jpeg\", \".webp\", \".gif\")\n",
        "    # for root, dirs, files in os.walk(imagesDirectory):\n",
        "    #     # Filter out image files and text files\n",
        "    #     image_files = [f for f in files if f.lower().endswith(image_extensions)]\n",
        "    #     txt_files = {f[:-4] for f in files if f.lower().endswith(\".txt\")}\n",
        "\n",
        "    #     for f in image_files:\n",
        "    #         # Check if the image file does not have a corresponding txt file\n",
        "    #         if f[:-4] not in txt_files:\n",
        "    #             image_paths.append(os.path.join(root, f))\n",
        "\n",
        "    for suspectIndex in range(len(imageSuspects)):\n",
        "        processedImages += 1\n",
        "        remainingImages = totalImages - processedImages\n",
        "        caption = \"\"\n",
        "        if processedImages == 1:\n",
        "            if showCaption:\n",
        "                print(\"-----\")\n",
        "            if showProcessedImages:\n",
        "                print(f\"Processed images: {processedImages}/{totalImages}\")\n",
        "        if (remainingImages % 2) == 0:\n",
        "            if showRemainingImages:\n",
        "                print(f\"Remaining images: {remainingImages} 🌘\")\n",
        "        else:\n",
        "            if showRemainingImages:\n",
        "                print(f\"Remaining images: {remainingImages} 🌖\")\n",
        "\n",
        "        imagePath = imageSuspects[suspectIndex]\n",
        "        imageFilePath = f\"{dirpath}/{imagePath}\"\n",
        "        textFilePath = f\"{Path(imageFilePath).with_suffix('')}.txt\"\n",
        "\n",
        "        rawImage = Image.open(imageFilePath).convert('RGB')\n",
        "        if overwriteExistingCaptions or not os.path.exists(textFilePath):\n",
        "            caption = generateCaption(rawImage, beams=3)\n",
        "            if useFolderNamesAsTokens:\n",
        "                caption = generateFolderNamesAsTokens(caption)\n",
        "            if appendStyles:\n",
        "                caption = generateExtraDescriptors(rawImage, caption)\n",
        "\n",
        "            with open(textFilePath, 'w+') as f:\n",
        "                f.write(caption)\n",
        "\n",
        "        if remainingImages > 0:\n",
        "            if clearOutput:\n",
        "                clear_output(wait=True)\n",
        "            if showCaption:\n",
        "                print(caption)\n",
        "            if showRecaptionedImages:\n",
        "                print(\"Recaptioned Images: \", recaptionedImages)\n",
        "            if showCurrentImage and os.path.exists(textFilePath):\n",
        "                display(rawImage.resize(( int(rawImage.width * 0.333), int(rawImage.height * 0.333))))\n",
        "        else:\n",
        "            if clearOutput:\n",
        "                clear_output(wait=True)\n",
        "            if showCaption:\n",
        "                print(caption)\n",
        "            if showRecaptionedImages:\n",
        "                print(\"Recaptioned Images: \", recaptionedImages)\n",
        "            if showProcessedImages:\n",
        "                print(f\"Processed images: {processedImages}/{totalImages}\")\n",
        "            if showRemainingImages:\n",
        "                print(f\"Remaining images: {remainingImages} ✅\" )\n",
        "            if showCurrentImage and os.path.exists(textFilePath):\n",
        "                display(rawImage.resize(( int(rawImage.width * 0.333), int(rawImage.height * 0.333))))\n",
        "\n",
        "# Iterate through directories inside directories\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    captionImages(dirpath)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "blip2demo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "50c50f1f55c832fff615d4f17bdf1949c2ce06a8f6fb0f097854f91355ce9518"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
