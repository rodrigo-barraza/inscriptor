{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKASCQg9nU5"
      },
      "source": [
        "# Rodrigo Barraza's Inscriptions: Blip 2 Mass Captioning\n",
        "Large RAM and VRAM is required to load the larger models. RAM should be at least 24-32GB with 64GB being optimal. VRAM should be at least 16GB or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mLI_ic0j9nU-",
        "outputId": "4b079d97-cddd-41ec-8936-23518f8a129a"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration, AutoTokenizer, Blip2Model\n",
        "import torch\n",
        "import sys\n",
        "import validators\n",
        "\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# from lavis.models import load_model_and_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Salesforce/blip2-opt-2.7b\n",
        "# Salesforce/blip2-opt-2.7b-coco\n",
        "# Salesforce/blip2-opt-6.7b\n",
        "# Salesforce/blip2-opt-6.7b-coco\n",
        "# Salesforce/blip2-flan-t5-xl\n",
        "# Salesforce/blip2-flan-t5-xl-coco\n",
        "# /Salesforce/blip2-flan-t5-xxl\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\")\n",
        "    # add_prefix_space=True) # Required to use bad_words_ids\n",
        "processor = Blip2Processor.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\",\n",
        "    device_map='auto',\n",
        "    # load_in_8bit=True)\n",
        "    torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# Start of Options\n",
        "# imagesDirectory = \"/mnt/d/dataset-1080\"\n",
        "# useFolderNamesAsTokens = True  # Append the folder names to the beginning of the caption\n",
        "# overwriteExistingCaptions = False  # Overwrite existing captions\n",
        "# tokensStartOrEnd = 'start'  # end or start\n",
        "# useNucleusSampling = False\n",
        "# appendStyles = True\n",
        "# showImages = False\n",
        "imagesDirectory = \"/mnt/d/fixes\"\n",
        "useFolderNamesAsTokens = True  # Append the folder names to the beginning of the caption\n",
        "overwriteExistingCaptions = True  # Overwrite existing captions\n",
        "tokensStartOrEnd = 'start'  # end or start\n",
        "appendStyles = True\n",
        "showImages = False\n",
        "\n",
        "# PREPROCESSOR SETTINGS\n",
        "skipSpecialTokens = True\n",
        "\n",
        "# MODEL GENERATION SETTINGS\n",
        "useNucleusSampling = False\n",
        "numberOfBeams = 3  # The number of beams to use for beam search\n",
        "lengthPenalty = 1\n",
        "minTokenLength = 5  # The amount of minimum tokens to generate\n",
        "maxTokenLength = 72  # The maximum amount of tokens to generate\n",
        "repetitionPenalty = 1\n",
        "topP = 0.9\n",
        "temperature = 1.0\n",
        "\n",
        "# TOKENIZER SETTINGS\n",
        "enableForceWords = False\n",
        "forceWordsList = [\"water\"]\n",
        "removeBadWords = False\n",
        "badWordsList = [\"teddy bear\"]\n",
        "padding = True\n",
        "add_special_tokens = False\n",
        "\n",
        "\n",
        "# prompt = \"Describe the style in 1 word. Answer:\"\n",
        "prompt = None\n",
        "# End of Options\n",
        "###############################################################################\n",
        "\n",
        "forceWordsIds = None\n",
        "badWordsIds = None\n",
        "# Count the total number of images in the directory and subdirectories\n",
        "totalImages = 0\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    totalImages += sum([filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\")) for filename in filenames])\n",
        "processedImages = 0\n",
        "\n",
        "def generateCaption(rawImage):\n",
        "    global forceWordsIds\n",
        "    generatedCaption = ''\n",
        "    if enableForceWords:\n",
        "        forceWords = tokenizer(\n",
        "            forceWordsList,\n",
        "            padding=padding,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            return_tensors=\"pt\").to(device).input_ids\n",
        "        forceWordsIds = forceWords.tolist()  # Convert the tensor to a list of lists\n",
        "\n",
        "    if removeBadWords:\n",
        "        badWords = tokenizer(\n",
        "            badWordsList,\n",
        "            padding=padding,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            return_tensors=\"pt\").to(device).input_ids\n",
        "        badWordsIds = badWords.tolist()  # Convert the tensor to a list of lists\n",
        "\n",
        "    inputs = processor(images=rawImage, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=useNucleusSampling,\n",
        "        force_words_ids=forceWordsIds,\n",
        "        # bad_words_ids=badWordsIds,\n",
        "        num_beams=numberOfBeams,\n",
        "        max_length=maxTokenLength,\n",
        "        min_length=minTokenLength,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        # num_return_sequences=5,\n",
        "        temperature=temperature)\n",
        "    imageCaption = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "\n",
        "    modifiedCaption = imageCaption[0]\n",
        "    captionWords = modifiedCaption.strip()\n",
        "    # Fix grammatical spelling errors by BLIP2\n",
        "    if \"laying\" in captionWords:\n",
        "        modifiedCaption = modifiedCaption.replace('laying', 'lying')\n",
        "    # Append/Prepend folder names to the caption\n",
        "    relpath = os.path.relpath(dirpath, imagesDirectory)\n",
        "    relpathParts = [part for part in relpath.split(os.sep) if \"_\" not in part and part != \".\"]\n",
        "    validParts = [part for part in relpathParts if part.lower() not in captionWords and part.lower() not in modifiedCaption]\n",
        "\n",
        "    if useFolderNamesAsTokens and validParts:\n",
        "        if tokensStartOrEnd == 'end':\n",
        "            generatedCaption = f\"{modifiedCaption}, {', '.join(validParts)}\"\n",
        "        else:\n",
        "            generatedCaption = f\"{', '.join(validParts)}, {modifiedCaption}\"\n",
        "    else:\n",
        "        generatedCaption = modifiedCaption.strip()\n",
        "    print(generatedCaption)\n",
        "    return cleanUpCaption(generatedCaption)\n",
        "\n",
        "def generateExtraDescriptors(rawImage, caption):\n",
        "    generatedExtraDescriptions = ''\n",
        "    inputs = processor(images=rawImage, text=\"Describe the style in 1 word. Answer:\", return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        max_length=10,\n",
        "        min_length=1,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    style = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "    inputs = processor(images=rawImage, text=\"Describe the theme in 1 word. Answer:\", return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        max_length=10,\n",
        "        min_length=1,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    theme = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "    inputs = processor(images=rawImage, text=\"Describe the medium in 1 word. Answer:\", return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        max_length=10,\n",
        "        min_length=1,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    medium = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "\n",
        "    combined = OrderedDict()\n",
        "    combinedLists = style + theme + medium\n",
        "    answers = [item.strip() for sublist in combinedLists for item in re.split(',|and', sublist) if item.strip()]\n",
        "\n",
        "    for answer in answers:\n",
        "        lowerWord = answer.lower().lstrip()\n",
        "\n",
        "        if '_' in lowerWord or '??' in lowerWord or '!!' in lowerWord or '—' in lowerWord or '~' in lowerWord or '@' in lowerWord or '|' in lowerWord:\n",
        "            lowerWord = ''\n",
        "\n",
        "        if lowerWord.startswith(('a ', 'the ', 'and ')):\n",
        "            lowerWord = lowerWord.split(' ', 1)[1]\n",
        "\n",
        "        if lowerWord.endswith(('.', ',', '!', '?')):\n",
        "            lowerWord = lowerWord[:-1]\n",
        "\n",
        "        if len(lowerWord) > 1:\n",
        "            combined[lowerWord] = None\n",
        "\n",
        "    uniqueCombinedArray = list(combined)\n",
        "\n",
        "    uniqueImageAnswers = set(answer.lower() for answer in uniqueCombinedArray)\n",
        "    filteredImageAnswers = [ans for ans in uniqueImageAnswers if not re.search(rf'\\b{re.escape(ans)}\\b', caption.lower())]\n",
        "\n",
        "    if filteredImageAnswers:\n",
        "        generatedExtraDescriptions = caption + ', ' + ', '.join(filteredImageAnswers)\n",
        "    return cleanUpCaption(generatedExtraDescriptions)\n",
        "\n",
        "def cleanUpCaption(caption):\n",
        "    cleanedUpCaption = caption\n",
        "    if \" - \" in caption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\n",
        "            \"t - shirt\", \"t-shirt\").replace(\n",
        "            \" - man\", \"-man\").replace(\n",
        "            \" - men\", \"-men\").replace(\n",
        "            \"t - rex\", \"t-rex\").replace(\n",
        "            \"sci - fi\", \"sci-fi\").replace(\n",
        "            \"x - files\", \"x-files\")\n",
        "\n",
        "    if \" - man\" in caption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\" - man\", \"-man\")\n",
        "    if \"http\" in caption or \"www\" in caption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"http\", \"\").replace(\"www\", \"\")\n",
        "    if \"/\" in caption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"/\", \"\")\n",
        "    if  \" - stock image\" in caption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\" - stock image\", \"\")\n",
        "    if \"sci - fi\" in caption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"sci - fi\", \"sci-fi\")\n",
        "    if \"t.v\" in caption or \"t.v.\" in caption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"t.v.\", \"television\").replace(\"t.v\", \"television\")\n",
        "    if \" & \" in caption and len(caption) == 5:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\" & \", \"&\")\n",
        "    if \"black\" in caption and \"white\" in caption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"black & white\", \"black and white\")\n",
        "\n",
        "    if \"blanka\" in caption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"hulk\", \"blanka\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"green hair\", \"orange hair\")\n",
        "    return cleanedUpCaption\n",
        "\n",
        "def captionImages(dirpath):\n",
        "    global processedImages\n",
        "    suspects = os.listdir(dirpath)\n",
        "    imageSuspects = [filename for filename in suspects if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\"))]\n",
        "\n",
        "    # Process each image\n",
        "    for suspectIndex in range(len(imageSuspects)):\n",
        "        processedImages += 1\n",
        "        remainingImages = totalImages - processedImages\n",
        "        caption = \"\"\n",
        "        print(f\"Processed images: {processedImages}/{totalImages}\")\n",
        "        print(f\"Remaining images: {remainingImages}\")\n",
        "\n",
        "        # Load Image\n",
        "        imagePath = imageSuspects[suspectIndex]\n",
        "        imageFilePath = f\"{dirpath}/{imagePath}\"\n",
        "        textFilePath = f\"{Path(imageFilePath).with_suffix('')}.txt\"\n",
        "\n",
        "        # If the image hasn't already been processed, caption it\n",
        "        if overwriteExistingCaptions or not os.path.exists(textFilePath):\n",
        "            rawImage = Image.open(imageFilePath).convert('RGB')\n",
        "            # Display the image as it's been processed\n",
        "            if showImages:\n",
        "                display(rawImage)\n",
        "                # display(image.resize(( int(image.width * 0.333), int(image.height * 0.333))))\n",
        "\n",
        "            caption = generateCaption(rawImage)\n",
        "            \n",
        "            if appendStyles:\n",
        "                caption = generateExtraDescriptors(rawImage, caption)\n",
        "\n",
        "            # Remove periods\n",
        "            # caption.replace('.', '')\n",
        "            # caption.replace('\"', '')\n",
        "\n",
        "            # Save Caption as .txt file\n",
        "            with open(textFilePath, 'w+') as f:\n",
        "                f.write(caption)\n",
        "        # clear_output(wait=True)\n",
        "        print(caption)\n",
        "\n",
        "\n",
        "# Iterate through directories inside directories\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    captionImages(dirpath)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "blip2demo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "50c50f1f55c832fff615d4f17bdf1949c2ce06a8f6fb0f097854f91355ce9518"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
