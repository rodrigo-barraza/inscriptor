{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKASCQg9nU5"
      },
      "source": [
        "# Rodrigo Barraza's Inscriptions: Blip 2 Mass Captioning\n",
        "Large RAM and VRAM is required to load the larger models. RAM should be at least 24-32GB with 64GB being optimal. VRAM should be at least 16GB or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mLI_ic0j9nU-",
        "outputId": "4b079d97-cddd-41ec-8936-23518f8a129a"
      },
      "outputs": [],
      "source": [
        "!pip3 install salesforce-lavis --upgrade\n",
        "!pip3 install validators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7c-uVWq9nVB",
        "outputId": "1ed1d302-bc0c-48fd-8e5c-4788f7577c58"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import validators\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from lavis.models import load_model_and_preprocess"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "85AsbJrs9nVF"
      },
      "source": [
        "#### Load BLIP2 captioning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "CAObkxLK9nVF",
        "outputId": "59ab7fa9-2410-4112-8a12-da164d742ae1"
      },
      "outputs": [],
      "source": [
        "# setup device to use\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "# we associate a model with its preprocessors to make it easier for inference.\n",
        "model, vis_processors, _ = load_model_and_preprocess(\n",
        "    # name=\"blip2_opt\", model_type=\"pretrain_opt2.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_opt\", model_type=\"pretrain_opt6.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_opt\", model_type=\"caption_coco_opt2.7b\", is_eval=True, device=device\n",
        "    name=\"blip2_opt\", model_type=\"caption_coco_opt6.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_t5\", model_type=\"pretrain_flant5xl\", is_eval=True, device=device\n",
        "    # name=\"blip2_t5\", model_type=\"caption_coco_flant5xl\", is_eval=True, device=device\n",
        "    # This next model is one scary devil in terms of size. It requires at least 32GB of VRAM to run, and will not load on 3090s or 4090s.\n",
        "    # name=\"blip2_t5\", model_type=\"pretrain_flant5xxl\", is_eval=True, device=device\n",
        ")\n",
        "\n",
        "vis_processors.keys()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wfO5NdZv9nVH"
      },
      "source": [
        "#### Auto Caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "photograph, elizabeth elder, medium wide shot, a woman in a pink dress with her hands folded\n",
            "/mnt/d/dataset-1080/photograph/_photographers/elizabeth elder/medium wide shot/0001840.jpg\n",
            "Processed images: 1839/12275\n",
            "Remaining images: 10436\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "\n",
        "# Start of Options\n",
        "imagesDirectory = \"/mnt/d/dataset-1080/\"\n",
        "useFolderNamesAsTokens = True  # Append the folder names to the beginning of the caption\n",
        "tokensStartOrEnd = 'start'  # end or start\n",
        "minTokenLength = 5  # The amount of minimum tokens to generate\n",
        "maxTokenLength = 72  # The maximum amount of tokens to generate\n",
        "numberOfCaptions = 1  # How many captions to generate\n",
        "useNucleusSampling = False\n",
        "repetitionPenalty = 1\n",
        "# End of Options\n",
        "\n",
        "# Count the total number of images in the directory and subdirectories\n",
        "total_images = 0\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    total_images += sum([filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\")) for filename in filenames])\n",
        "\n",
        "processed_images = 0\n",
        "\n",
        "\n",
        "def process_images(dirpath):\n",
        "    global processed_images\n",
        "    suspects = os.listdir(dirpath)\n",
        "    imageSuspects = [filename for filename in suspects if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\"))]\n",
        "\n",
        "    for suspectIndex in range(len(imageSuspects)):\n",
        "        processed_images += 1\n",
        "        remaining_images = total_images - processed_images\n",
        "        caption = \"\"\n",
        "        print(f\"Processed images: {processed_images}/{total_images}\")\n",
        "        print(f\"Remaining images: {remaining_images}\")\n",
        "\n",
        "        # Load Image\n",
        "        imagePath = imageSuspects[suspectIndex]\n",
        "        imageFilePath = dirpath + \"/\" + imagePath\n",
        "        textFilePath = Path(imageFilePath).with_suffix('.txt')\n",
        "\n",
        "        # If file doesn't exist:\n",
        "        if not os.path.exists(textFilePath):\n",
        "            rawImage = Image.open(imageFilePath).convert('RGB')\n",
        "            # display(rawImage)\n",
        "            image = vis_processors[\"eval\"](rawImage).unsqueeze(0).to(device)\n",
        "            imageCaption = model.generate({\"image\": image}, min_length=minTokenLength, max_length=maxTokenLength, use_nucleus_sampling=useNucleusSampling, num_captions=numberOfCaptions, repetition_penalty=repetitionPenalty)\n",
        "            modifiedCaption = imageCaption[0]\n",
        "            captionWords = modifiedCaption.split()\n",
        "\n",
        "            # Fix grammatical spelling errors by BLIP2\n",
        "            if \"laying\" in captionWords:\n",
        "                modifiedCaption = modifiedCaption.replace('laying', 'lying')\n",
        "\n",
        "            if useFolderNamesAsTokens:\n",
        "                relpath = os.path.relpath(dirpath, imagesDirectory)\n",
        "                relpath_parts = relpath.split(os.sep)\n",
        "\n",
        "                if tokensStartOrEnd == 'end':\n",
        "                    caption += modifiedCaption\n",
        "                    for part in relpath_parts:\n",
        "                        if \"_\" not in part and part != \".\":\n",
        "                            imageCaptionSet = set(captionWords)\n",
        "                            if part not in imageCaptionSet and part not in modifiedCaption:\n",
        "                                caption += \", \" + part\n",
        "                else:\n",
        "                    caption = \"\"\n",
        "                    for part in relpath_parts:\n",
        "                        if \"_\" not in part and part != \".\":\n",
        "                            if part.startswith('-'):\n",
        "                                imageCaptionSet = set(captionWords)\n",
        "                                if part.replace('-', '') not in imageCaptionSet and part.replace('-', '') not in modifiedCaption:\n",
        "                                    caption += part.replace('-', '') + \", \"\n",
        "                            else:\n",
        "                                caption += part + \", \"\n",
        "                    caption += modifiedCaption\n",
        "            else:\n",
        "                caption = imageCaption[0]\n",
        "\n",
        "            # Remove periods\n",
        "            caption.replace('.', '')\n",
        "\n",
        "            # Save Caption as .txt file\n",
        "            with open(textFilePath, 'w+') as f:\n",
        "                f.write(caption)\n",
        "        clear_output(wait=True)\n",
        "        print(caption)\n",
        "        print(imageFilePath)\n",
        "                     \n",
        "# Iterate through directories inside directories\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    process_images(dirpath)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "blip2demo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "50c50f1f55c832fff615d4f17bdf1949c2ce06a8f6fb0f097854f91355ce9518"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
