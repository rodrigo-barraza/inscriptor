{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKASCQg9nU5"
      },
      "source": [
        "# Rodrigo Barraza's Inscriptions: Blip 2 Mass Captioning\n",
        "Large RAM and VRAM is required to load the larger models. RAM should be at least 24-32GB with 64GB being optimal. VRAM should be at least 16GB or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mLI_ic0j9nU-",
        "outputId": "4b079d97-cddd-41ec-8936-23518f8a129a"
      },
      "outputs": [],
      "source": [
        "!pip3 install salesforce-lavis --upgrade\n",
        "!pip3 install validators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7c-uVWq9nVB",
        "outputId": "1ed1d302-bc0c-48fd-8e5c-4788f7577c58"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import validators\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from lavis.models import load_model_and_preprocess"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "85AsbJrs9nVF"
      },
      "source": [
        "#### Load BLIP2 captioning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "CAObkxLK9nVF",
        "outputId": "59ab7fa9-2410-4112-8a12-da164d742ae1"
      },
      "outputs": [],
      "source": [
        "# setup device to use\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "# we associate a model with its preprocessors to make it easier for inference.\n",
        "model, vis_processors, _ = load_model_and_preprocess(\n",
        "    # name=\"blip2_opt\", model_type=\"pretrain_opt2.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_opt\", model_type=\"pretrain_opt6.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_opt\", model_type=\"caption_coco_opt2.7b\", is_eval=True, device=device\n",
        "    name=\"blip2_opt\", model_type=\"caption_coco_opt6.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_t5\", model_type=\"pretrain_flant5xl\", is_eval=True, device=device\n",
        "    # name=\"blip2_t5\", model_type=\"caption_coco_flant5xl\", is_eval=True, device=device\n",
        "    # This next model is one scary devil in terms of size...\n",
        "    # ... it requires at least 32GB of VRAM to run...\n",
        "    # ... and will not load on 3090s or 4090s.\n",
        "    # name=\"blip2_t5\", model_type=\"pretrain_flant5xxl\", is_eval=True, device=device\n",
        ")\n",
        "\n",
        "vis_processors.keys()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wfO5NdZv9nVH"
      },
      "source": [
        "#### Auto Caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "###############################################################################\n",
        "# Start of Options\n",
        "imagesDirectory = \"/mnt/d/datatest/\"\n",
        "useFolderNamesAsTokens = True  # Append the folder names to the beginning of the caption\n",
        "tokensStartOrEnd = 'start'  # end or start\n",
        "minTokenLength = 15  # The amount of minimum tokens to generate\n",
        "maxTokenLength = 20  # The maximum amount of tokens to generate\n",
        "\n",
        "useNucleusSampling = False\n",
        "repetitionPenalty = 1\n",
        "\n",
        "appendStyles = True\n",
        "promptQuestion = \"Describe the style in 1 word\"\n",
        "numberOfAnswers = 5\n",
        "minAnswerLength = 7\n",
        "maxAnswerLength = 10\n",
        "\n",
        "showImages = False\n",
        "# End of Options\n",
        "###############################################################################\n",
        "\n",
        "numberOfCaptions = 1  # How many captions to generate\n",
        "# Count the total number of images in the directory and subdirectories\n",
        "totalImages = 0\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    totalImages += sum([filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\")) for filename in filenames])\n",
        "\n",
        "processedImages = 0\n",
        "\n",
        "\n",
        "def process_images(dirpath):\n",
        "    global processedImages\n",
        "    suspects = os.listdir(dirpath)\n",
        "    imageSuspects = [filename for filename in suspects if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\"))]\n",
        "\n",
        "    # Process each image\n",
        "    for suspectIndex in range(len(imageSuspects)):\n",
        "        processedImages += 1\n",
        "        remainingImages = totalImages - processedImages\n",
        "        caption = \"\"\n",
        "        print(f\"Processed images: {processedImages}/{totalImages}\")\n",
        "        print(f\"Remaining images: {remainingImages}\")\n",
        "\n",
        "        # Load Image\n",
        "        imagePath = imageSuspects[suspectIndex]\n",
        "        imageFilePath = dirpath + \"/\" + imagePath\n",
        "        textFilePath = Path(imageFilePath).with_suffix('.txt')\n",
        "\n",
        "        # If the image hasn't already been processed, caption it\n",
        "        if not os.path.exists(textFilePath):\n",
        "            rawImage = Image.open(imageFilePath).convert('RGB')\n",
        "            # Display the image as it's been processed\n",
        "            if showImages:\n",
        "                display(rawImage)\n",
        "            image = vis_processors[\"eval\"](rawImage).unsqueeze(0).to(device)\n",
        "            imageCaption = model.generate({\"image\": image}, min_length=minTokenLength, max_length=maxTokenLength, use_nucleus_sampling=useNucleusSampling, num_captions=numberOfCaptions, repetition_penalty=repetitionPenalty)\n",
        "\n",
        "            modifiedCaption = imageCaption[0]\n",
        "            captionWords = modifiedCaption.split()\n",
        "\n",
        "            # Fix grammatical spelling errors by BLIP2\n",
        "            if \"laying\" in captionWords:\n",
        "                modifiedCaption = modifiedCaption.replace('laying', 'lying')\n",
        "\n",
        "            # Append/Prepend folder names to the caption\n",
        "            if useFolderNamesAsTokens:\n",
        "                relpath = os.path.relpath(dirpath, imagesDirectory)\n",
        "                relpathParts = [part for part in relpath.split(os.sep) if \"_\" not in part and part != \".\"]\n",
        "\n",
        "                validParts = [part for part in relpathParts if part.lower() not in captionWords and part.lower() not in modifiedCaption]\n",
        "\n",
        "                if tokensStartOrEnd == 'end':\n",
        "                    caption = f\"{modifiedCaption}, {', '.join(validParts)}\"\n",
        "                else:\n",
        "                    caption = f\"{', '.join(validParts)}, {modifiedCaption}\"\n",
        "            else:\n",
        "                caption = imageCaption[0]\n",
        "\n",
        "            # Append answers to the caption\n",
        "            if appendStyles:\n",
        "                style = model.generate({\"image\": image, \"prompt\": \"Describe the style in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=3, min_length=7, max_length=10)\n",
        "                theme = model.generate({\"image\": image, \"prompt\": \"Describe the theme in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=3, min_length=7, max_length=10)\n",
        "                background = model.generate({\"image\": image, \"prompt\": \"Describe object in the background in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=5, min_length=7, max_length=10)\n",
        "                medium = model.generate({\"image\": image, \"prompt\": \"Describe the medium in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=3, min_length=7, max_length=10)\n",
        "                color = model.generate({\"image\": image, \"prompt\": \"Describe the color in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=5, min_length=7, max_length=10)\n",
        "                person = model.generate({\"image\": image, \"prompt\": \"Describe the person in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=5, min_length=7, max_length=10)\n",
        "                outfit = model.generate({\"image\": image, \"prompt\": \"Describe the outfit in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=5, min_length=7, max_length=10)\n",
        "                \n",
        "                combined = OrderedDict()\n",
        "\n",
        "                answers = [person, outfit, background, style, theme, medium, color]\n",
        "\n",
        "                for answer in answers:\n",
        "                    for word in answer:\n",
        "                        lowerWord = word.lower().lstrip()\n",
        "\n",
        "                        if '_' in lowerWord:\n",
        "                            lowerWord = ''\n",
        "\n",
        "                        if lowerWord.startswith(('a ', 'the ', 'and ')):\n",
        "                            lowerWord = lowerWord.split(' ', 1)[1]\n",
        "\n",
        "                        if lowerWord.endswith(('.', ',', '!', '?')):\n",
        "                            lowerWord = lowerWord[:-1]\n",
        "\n",
        "                        lowerWord = lowerWord.replace(', and ', ' and ').replace(', ', ' and ')\n",
        "\n",
        "                        if len(lowerWord) > 1:\n",
        "                            combined[lowerWord] = None\n",
        "\n",
        "                uniqueCombinedArray = list(combined)\n",
        "\n",
        "                uniqueImageAnswers = set(answer.lower() for answer in uniqueCombinedArray)\n",
        "                filteredImageAnswers = [ans for ans in uniqueImageAnswers if not re.search(rf'\\b{ans}\\b', caption.lower())]\n",
        "\n",
        "                if filteredImageAnswers:\n",
        "                    caption += ', ' + ', '.join(filteredImageAnswers)\n",
        "\n",
        "            # Remove periods\n",
        "            caption.replace('.', '')\n",
        "\n",
        "            # Save Caption as .txt file\n",
        "            with open(textFilePath, 'w+') as f:\n",
        "                f.write(caption)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        print(caption)\n",
        "        print(imageFilePath)\n",
        "                     \n",
        "# Iterate through directories inside directories\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    process_images(dirpath)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "blip2demo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "50c50f1f55c832fff615d4f17bdf1949c2ce06a8f6fb0f097854f91355ce9518"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
