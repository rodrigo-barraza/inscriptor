{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKASCQg9nU5"
      },
      "source": [
        "# Rodrigo Barraza's Inscriptions: Blip 2 Mass Captioning\n",
        "Large RAM and VRAM is required to load the larger models. RAM should be at least 24-32GB with 64GB being optimal. VRAM should be at least 16GB or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mLI_ic0j9nU-",
        "outputId": "4b079d97-cddd-41ec-8936-23518f8a129a"
      },
      "outputs": [],
      "source": [
        "!pip3 install salesforce-lavis --upgrade\n",
        "!pip3 install validators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7c-uVWq9nVB",
        "outputId": "1ed1d302-bc0c-48fd-8e5c-4788f7577c58"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import validators\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from lavis.models import load_model_and_preprocess"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "85AsbJrs9nVF"
      },
      "source": [
        "#### Load BLIP2 captioning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "CAObkxLK9nVF",
        "outputId": "59ab7fa9-2410-4112-8a12-da164d742ae1"
      },
      "outputs": [],
      "source": [
        "# setup device to use\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "# we associate a model with its preprocessors to make it easier for inference.\n",
        "model, vis_processors, _ = load_model_and_preprocess(\n",
        "    # name=\"blip2_opt\", model_type=\"pretrain_opt2.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_opt\", model_type=\"pretrain_opt6.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_opt\", model_type=\"caption_coco_opt2.7b\", is_eval=True, device=device\n",
        "    name=\"blip2_opt\", model_type=\"caption_coco_opt6.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_t5\", model_type=\"pretrain_flant5xl\", is_eval=True, device=device\n",
        "    # name=\"blip2_t5\", model_type=\"caption_coco_flant5xl\", is_eval=True, device=device\n",
        "    # This next model is one scary devil in terms of size. It requires at least 32GB of VRAM to run, and will not load on 3090s or 4090s.\n",
        "    # name=\"blip2_t5\", model_type=\"pretrain_flant5xxl\", is_eval=True, device=device\n",
        ")\n",
        "\n",
        "vis_processors.keys()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wfO5NdZv9nVH"
      },
      "source": [
        "#### Auto Caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Start of Options\n",
        "imagesDirectory = \"/mnt/c/dataset\"\n",
        "useFoldersAsTokens = True # Append the folder names to the beginning of the caption\n",
        "minTokenLength = 5 # The amount of minimum tokens to generate\n",
        "maxTokenLength = 72 # The maximum amount of tokens to generate\n",
        "numberOfCaptions = 1 # How many captions to generate\n",
        "useNucleusSampling = False\n",
        "repetitionPenalty = 1\n",
        "# End of Options\n",
        "\n",
        "\n",
        "def process_images(dirpath):\n",
        "    suspects = os.listdir(dirpath) \n",
        "    imageSuspects = [filename for filename in suspects if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\"))]\n",
        "\n",
        "    for suspectIndex in range(len(imageSuspects)):\n",
        "        caption = \"\"\n",
        "        suspectsLength = len(imageSuspects) + 1\n",
        "        print(f\"{suspectsLength - suspectIndex}/{suspectsLength}\")\n",
        "        # Load Image\n",
        "        imagePath = imageSuspects[suspectIndex]\n",
        "        imageFilePath = dirpath + \"/\" + imagePath\n",
        "        textFilePath = Path(imageFilePath).with_suffix('.txt')\n",
        "        # If file doesn't exist:\n",
        "        if not os.path.exists(textFilePath):\n",
        "            rawImage = Image.open(imageFilePath).convert('RGB')\n",
        "            # display(rawImage)\n",
        "            image = vis_processors[\"eval\"](rawImage).unsqueeze(0).to(device)\n",
        "            imageCaption = model.generate({\"image\": image}, min_length=minTokenLength, max_length=maxTokenLength, use_nucleus_sampling=useNucleusSampling, num_captions=numberOfCaptions, repetition_penalty=repetitionPenalty)\n",
        "            if useFoldersAsTokens:\n",
        "                relpath = os.path.relpath(dirpath, imagesDirectory)\n",
        "                relpath_parts = relpath.split(os.sep)\n",
        "                caption += imageCaption[0]\n",
        "                for part in relpath_parts:\n",
        "                    if \"_\" not in part:\n",
        "                        cleanPart = part.replace('-', ' ')\n",
        "                        if cleanPart not in caption:\n",
        "                            caption += \", \" + cleanPart\n",
        "            else:\n",
        "                caption = imageCaption[0]\n",
        "            # Remove periods\n",
        "            caption.replace('.', '')\n",
        "            # Save Caption as .txt file\n",
        "            with open(textFilePath, 'w+') as f:\n",
        "                f.write(caption)\n",
        "        clear_output(wait=True)\n",
        "        print(caption)\n",
        "        print(imageFilePath)\n",
        "\n",
        "# Iterate through directories inside directories\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    process_images(dirpath)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "blip2demo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "50c50f1f55c832fff615d4f17bdf1949c2ce06a8f6fb0f097854f91355ce9518"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
