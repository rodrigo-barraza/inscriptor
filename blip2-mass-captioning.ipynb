{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKASCQg9nU5"
      },
      "source": [
        "# Rodrigo Barraza's Inscriptions: Blip 2 Mass Captioning\n",
        "Large RAM and VRAM is required to load the larger models. RAM should be at least 24-32GB with 64GB being optimal. VRAM should be at least 16GB or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mLI_ic0j9nU-",
        "outputId": "4b079d97-cddd-41ec-8936-23518f8a129a"
      },
      "outputs": [],
      "source": [
        "!pip3 install salesforce-lavis --upgrade\n",
        "!pip3 install validators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7c-uVWq9nVB",
        "outputId": "1ed1d302-bc0c-48fd-8e5c-4788f7577c58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rodrigo/miniconda3/envs/blip2demo/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "WARNING:root:Pytorch pre-release version 2.1.0.dev20230412+cu118 - assuming intent to test it\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import validators\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from lavis.models import load_model_and_preprocess"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "85AsbJrs9nVF"
      },
      "source": [
        "#### Load BLIP2 captioning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "CAObkxLK9nVF",
        "outputId": "59ab7fa9-2410-4112-8a12-da164d742ae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Position interpolate from 16x16 to 26x26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rodrigo/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:402: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  with safe_open(checkpoint_file, framework=\"pt\") as f:\n"
          ]
        }
      ],
      "source": [
        "# setup device to use\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "# we associate a model with its preprocessors to make it easier for inference.\n",
        "model, vis_processors, _ = load_model_and_preprocess(\n",
        "    # name=\"blip2_opt\", model_type=\"pretrain_opt2.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_opt\", model_type=\"pretrain_opt6.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_opt\", model_type=\"caption_coco_opt2.7b\", is_eval=True, device=device\n",
        "    name=\"blip2_opt\", model_type=\"caption_coco_opt6.7b\", is_eval=True, device=device\n",
        "    # name=\"blip2_t5\", model_type=\"pretrain_flant5xl\", is_eval=True, device=device\n",
        "    # name=\"blip2_t5\", model_type=\"caption_coco_flant5xl\", is_eval=True, device=device\n",
        "    # This next model is one scary devil in terms of size...\n",
        "    # ... it requires at least 32GB of VRAM to run...\n",
        "    # ... and will not load on 3090s or 4090s.\n",
        "    # name=\"blip2_t5\", model_type=\"pretrain_flant5xxl\", is_eval=True, device=device\n",
        ")\n",
        "\n",
        "vis_processors.keys()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wfO5NdZv9nVH"
      },
      "source": [
        "#### Auto Caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "/mnt/d/datatest/spacescape/solar eclipse/0018301.jpeg\n",
            "Processed images: 102/225\n",
            "Remaining images: 123\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'vis_processors' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 149\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39m# Iterate through directories inside directories\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mfor\u001b[39;00m dirpath, dirnames, filenames \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mwalk(imagesDirectory):\n\u001b[0;32m--> 149\u001b[0m     process_images(dirpath)\n",
            "Cell \u001b[0;32mIn[1], line 61\u001b[0m, in \u001b[0;36mprocess_images\u001b[0;34m(dirpath)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m showImages:\n\u001b[1;32m     60\u001b[0m     display(rawImage)\n\u001b[0;32m---> 61\u001b[0m image \u001b[39m=\u001b[39m vis_processors[\u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m](rawImage)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     62\u001b[0m imageCaption \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate({\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m: image}, min_length\u001b[39m=\u001b[39mminTokenLength, max_length\u001b[39m=\u001b[39mmaxTokenLength, use_nucleus_sampling\u001b[39m=\u001b[39museNucleusSampling, num_captions\u001b[39m=\u001b[39mnumberOfCaptions, repetition_penalty\u001b[39m=\u001b[39mrepetitionPenalty)\n\u001b[1;32m     64\u001b[0m modifiedCaption \u001b[39m=\u001b[39m imageCaption[\u001b[39m0\u001b[39m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vis_processors' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "###############################################################################\n",
        "# Start of Options\n",
        "imagesDirectory = \"/mnt/d/datatest/\"\n",
        "useFolderNamesAsTokens = True  # Append the folder names to the beginning of the caption\n",
        "tokensStartOrEnd = 'start'  # end or start\n",
        "minTokenLength = 15  # The amount of minimum tokens to generate\n",
        "maxTokenLength = 20  # The maximum amount of tokens to generate\n",
        "\n",
        "useNucleusSampling = False\n",
        "repetitionPenalty = 1\n",
        "\n",
        "appendStyles = True\n",
        "promptQuestion = \"Describe the style in 1 word\"\n",
        "numberOfAnswers = 5\n",
        "minAnswerLength = 7\n",
        "maxAnswerLength = 10\n",
        "\n",
        "showImages = False\n",
        "# End of Options\n",
        "###############################################################################\n",
        "\n",
        "numberOfCaptions = 1  # How many captions to generate\n",
        "# Count the total number of images in the directory and subdirectories\n",
        "total_images = 0\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    total_images += sum([filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\")) for filename in filenames])\n",
        "\n",
        "processed_images = 0\n",
        "\n",
        "\n",
        "def process_images(dirpath):\n",
        "    global processed_images\n",
        "    suspects = os.listdir(dirpath)\n",
        "    imageSuspects = [filename for filename in suspects if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".webp\", \"gif\"))]\n",
        "\n",
        "    # Process each image\n",
        "    for suspectIndex in range(len(imageSuspects)):\n",
        "        processed_images += 1\n",
        "        remaining_images = total_images - processed_images\n",
        "        caption = \"\"\n",
        "        print(f\"Processed images: {processed_images}/{total_images}\")\n",
        "        print(f\"Remaining images: {remaining_images}\")\n",
        "\n",
        "        # Load Image\n",
        "        imagePath = imageSuspects[suspectIndex]\n",
        "        imageFilePath = dirpath + \"/\" + imagePath\n",
        "        textFilePath = Path(imageFilePath).with_suffix('.txt')\n",
        "\n",
        "        # If the image hasn't already been processed, caption it\n",
        "        if not os.path.exists(textFilePath):\n",
        "            rawImage = Image.open(imageFilePath).convert('RGB')\n",
        "            # Display the image as it's been processed\n",
        "            if showImages:\n",
        "                display(rawImage)\n",
        "            image = vis_processors[\"eval\"](rawImage).unsqueeze(0).to(device)\n",
        "            imageCaption = model.generate({\"image\": image}, min_length=minTokenLength, max_length=maxTokenLength, use_nucleus_sampling=useNucleusSampling, num_captions=numberOfCaptions, repetition_penalty=repetitionPenalty)\n",
        "\n",
        "            modifiedCaption = imageCaption[0]\n",
        "            captionWords = modifiedCaption.split()\n",
        "\n",
        "            # Fix grammatical spelling errors by BLIP2\n",
        "            if \"laying\" in captionWords:\n",
        "                modifiedCaption = modifiedCaption.replace('laying', 'lying')\n",
        "\n",
        "            # Append/Prepend folder names to the caption\n",
        "            if useFolderNamesAsTokens:\n",
        "                relpath = os.path.relpath(dirpath, imagesDirectory)\n",
        "                relpath_parts = relpath.split(os.sep)\n",
        "                # Append names to the caption\n",
        "                if tokensStartOrEnd == 'end':\n",
        "                    caption += modifiedCaption\n",
        "                    for part in relpath_parts:\n",
        "                        if \"_\" not in part and part != \".\":\n",
        "                            if part.lower() not in captionWords and part.lower() not in modifiedCaption:\n",
        "                                caption += \", \" + part\n",
        "                # Prepend names to the caption\n",
        "                else:\n",
        "                    caption = \"\"\n",
        "                    for part in relpath_parts:\n",
        "                        if \"_\" not in part and part != \".\":\n",
        "                            if part.lower() not in captionWords:\n",
        "                                caption += part + \", \"\n",
        "\n",
        "                    caption += modifiedCaption\n",
        "            # Use the caption as is\n",
        "            else:\n",
        "                caption = imageCaption[0]\n",
        "\n",
        "            # Append answers to the caption\n",
        "            if appendStyles:\n",
        "                style = model.generate({\"image\": image, \"prompt\": \"Describe the style in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=3, min_length=7, max_length=10)\n",
        "                theme = model.generate({\"image\": image, \"prompt\": \"Describe the theme in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=3, min_length=7, max_length=10)\n",
        "                background = model.generate({\"image\": image, \"prompt\": \"Describe object in the background in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=5, min_length=7, max_length=10)\n",
        "                medium = model.generate({\"image\": image, \"prompt\": \"Describe the medium in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=3, min_length=7, max_length=10)\n",
        "                color = model.generate({\"image\": image, \"prompt\": \"Describe the color in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=5, min_length=7, max_length=10)\n",
        "                person = model.generate({\"image\": image, \"prompt\": \"Describe the person in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=5, min_length=7, max_length=10)\n",
        "                outfit = model.generate({\"image\": image, \"prompt\": \"Describe the outfit in 1 word. Answer:\"}, use_nucleus_sampling=False, num_captions=5, min_length=7, max_length=10)\n",
        "                \n",
        "                combined = OrderedDict()\n",
        "\n",
        "                for array in [person, outfit, background, style, theme, medium, color]:\n",
        "                    for word in array:\n",
        "                        lower_word = word.lower()\n",
        "                        lower_word = lower_word.lstrip()  # Remove leading whitespace\n",
        "\n",
        "                        if '_' in lower_word:\n",
        "                            lower_word = ''\n",
        "\n",
        "                        if lower_word.startswith('a '):\n",
        "                            lower_word = lower_word[2:]  # Remove 'a ' from the beginning of the string\n",
        "                        elif lower_word.startswith('the ') or lower_word.startswith('and '):\n",
        "                            lower_word = lower_word[4:]  # Remove 'the ' from the beginning of the string\n",
        "\n",
        "                        if lower_word.endswith('.') or lower_word.endswith(',') or lower_word.endswith('!') or lower_word.endswith('?'):\n",
        "                            lower_word = lower_word[:-1]  # Remove the period from the end of the string\n",
        "\n",
        "                        if ', and ' in lower_word:\n",
        "                            lower_word = lower_word.replace(', and ', ' and ')\n",
        "                        if ', ' in lower_word:\n",
        "                            lower_word = lower_word.replace(', ', ' and ')\n",
        "\n",
        "                        split_words = [lower_word]\n",
        "\n",
        "                        for split_word in split_words:\n",
        "                            if split_word and len(split_word) > 1:  # This checks if the string is not empty and longer than 1 character:\n",
        "                                combined[split_word] = None\n",
        "\n",
        "                unique_combined_array = list(combined)\n",
        "\n",
        "                uniqueImageAnswers = set(answer.lower() for answer in unique_combined_array)\n",
        "                filteredImageAnswers = [answer for answer in uniqueImageAnswers if not re.search(r'\\b' + answer + r'\\b', caption.lower())]\n",
        "                if filteredImageAnswers:\n",
        "                    caption += ', ' + ', '.join(filteredImageAnswers)\n",
        "\n",
        "            # Remove periods\n",
        "            caption.replace('.', '')\n",
        "\n",
        "            # Save Caption as .txt file\n",
        "            with open(textFilePath, 'w+') as f:\n",
        "                f.write(caption)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        print(caption)\n",
        "        print(imageFilePath)\n",
        "                     \n",
        "# Iterate through directories inside directories\n",
        "for dirpath, dirnames, filenames in os.walk(imagesDirectory):\n",
        "    process_images(dirpath)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "blip2demo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "50c50f1f55c832fff615d4f17bdf1949c2ce06a8f6fb0f097854f91355ce9518"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
