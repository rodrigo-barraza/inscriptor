{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKASCQg9nU5"
      },
      "source": [
        "# Rodrigo Barraza's Inscriptions: Blip 2 Mass Captioning\n",
        "Large RAM and VRAM is required to load the larger models. RAM should be at least 24-32GB with 64GB being optimal. VRAM should be at least 16GB or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mLI_ic0j9nU-",
        "outputId": "4b079d97-cddd-41ec-8936-23518f8a129a"
      },
      "outputs": [],
      "source": [
        "# !pip3 install transformers\n",
        "# !pip install pillow\n",
        "# !pip install requests\n",
        "# !pip install validators\n",
        "# !pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration, AutoTokenizer, Blip2Model\n",
        "import torch\n",
        "import sys\n",
        "import validators\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# from lavis.models import load_model_and_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Salesforce/blip2-opt-2.7b\n",
        "# Salesforce/blip2-opt-2.7b-coco\n",
        "# Salesforce/blip2-opt-6.7b\n",
        "# Salesforce/blip2-opt-6.7b-coco\n",
        "# Salesforce/blip2-flan-t5-xl\n",
        "# Salesforce/blip2-flan-t5-xl-coco\n",
        "# /Salesforce/blip2-flan-t5-xxl\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\",\n",
        "    add_prefix_space=True) # Required to use bad_words_ids\n",
        "processor = Blip2Processor.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-6.7b-coco\",\n",
        "    device_map='auto',\n",
        "    # load_in_8bit=True)\n",
        "    torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hello: int = 0\n",
        "print(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# Start of Options\n",
        "\n",
        "imagesDirectory: str = r\"/mnt/d/dataset-1080-new\"\n",
        "addFolderNamesToCaptions: bool = True  # Append the folder names to the beginning of the caption\n",
        "folderNamesAtStartOrEnd: str = 'start'  # end or start\n",
        "enableOverwritingExistingCaptions: bool = False  # Overwrite existing captions\n",
        "enableExtraDescriptors: bool = True\n",
        "\n",
        "# PRINT OPTIONS\n",
        "showCurrentImage = True\n",
        "showCaption = True\n",
        "showRecaptionedImages = True\n",
        "showProcessedImages = True\n",
        "showRemainingImages = True\n",
        "clearOutput = True\n",
        "\n",
        "# PREPROCESSOR SETTINGS\n",
        "skipSpecialTokens = True\n",
        "\n",
        "# MODEL GENERATION SETTINGS\n",
        "useNucleusSampling = False\n",
        "numberOfBeams = 3  # The number of beams to use for beam search\n",
        "lengthPenalty = 1\n",
        "minTokenLength = 5  # The amount of minimum tokens to generate\n",
        "maxTokenLength = 72  # The maximum amount of tokens to generate\n",
        "repetitionPenalty = 1\n",
        "topP = 0.9\n",
        "temperature = 1.0\n",
        "\n",
        "# TOKENIZER SETTINGS\n",
        "# enableForceWords = False\n",
        "# forceWordsList = [\"water\"]\n",
        "# removeBadWords = False\n",
        "# badWordsList = [\"teddy bear\"]\n",
        "# padding = True\n",
        "# add_special_tokens = False\n",
        "\n",
        "# End of Options\n",
        "###############################################################################\n",
        "\n",
        "forceWordsIds = None\n",
        "badWordsIds = None\n",
        "primaryCaptionTokenLength = 0\n",
        "\n",
        "# Count the total number of images in the directory and subdirectories\n",
        "totalImages: int = 0\n",
        "processedImages: int = 0\n",
        "recaptionedImages: int = 0\n",
        "\n",
        "def generateCaption(rawImage, beams=numberOfBeams):\n",
        "    global forceWordsIds\n",
        "    global badWordsIds\n",
        "    global recaptionedImages\n",
        "    generatedCaption = ''\n",
        "\n",
        "    # if enableForceWords:\n",
        "    #     forceWords = tokenizer(\n",
        "    #         forceWordsList,\n",
        "    #         padding=padding,\n",
        "    #         add_special_tokens=add_special_tokens,\n",
        "    #         return_tensors=\"pt\").to(device).input_ids\n",
        "    #     forceWordsIds = forceWords.tolist()  # Convert the tensor to a list of lists\n",
        "\n",
        "    # if removeBadWords:\n",
        "    #     badWords = tokenizer(\n",
        "    #         badWordsList,\n",
        "    #         padding=padding,\n",
        "    #         add_special_tokens=add_special_tokens,\n",
        "    #         return_tensors=\"pt\").to(device).input_ids\n",
        "    #     badWordsIds = badWords.tolist()  # Convert the tensor to a list of lists\n",
        "\n",
        "    inputs = processor(images=rawImage, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=useNucleusSampling,\n",
        "        force_words_ids=forceWordsIds,\n",
        "        bad_words_ids=badWordsIds,\n",
        "        num_beams=beams,\n",
        "        max_length=maxTokenLength,\n",
        "        min_length=minTokenLength,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    primaryCaptionTokenLength = torch.numel(generated_ids)\n",
        "    # If token length is too long (because it's repeated words over and over), try again with different settings\n",
        "    if primaryCaptionTokenLength >= 72:\n",
        "        print(\"Recaptioning... âš ï¸\")\n",
        "        recaptionedImages += 1\n",
        "        generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=useNucleusSampling,\n",
        "        force_words_ids=forceWordsIds,\n",
        "        bad_words_ids=badWordsIds,\n",
        "        num_beams=10,\n",
        "        max_length=maxTokenLength,\n",
        "        min_length=minTokenLength,\n",
        "        repetition_penalty=1.5,\n",
        "        length_penalty=-1,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    generatedCaption = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "    generatedCaption = generatedCaption[0].strip()\n",
        "    # If token length is too long (because it's repeated words over and over), try again with different settings\n",
        "    if \"png\" in generatedCaption or \"download\" in generatedCaption:\n",
        "        print(\"Recaptioning... âš ï¸\")\n",
        "        badWords = tokenizer(\n",
        "            ['png', 'download'],\n",
        "            padding=True,\n",
        "            add_special_tokens=False,\n",
        "            return_tensors=\"pt\").to(device).input_ids\n",
        "        badWordsIds = badWords.tolist()\n",
        "        recaptionedImages += 1\n",
        "        generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=useNucleusSampling,\n",
        "        force_words_ids=forceWordsIds,\n",
        "        bad_words_ids=badWordsIds,\n",
        "        num_beams=10,\n",
        "        max_length=maxTokenLength,\n",
        "        min_length=minTokenLength,\n",
        "        repetition_penalty=1.5,\n",
        "        length_penalty=-1,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    generatedCaption = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "    generatedCaption = generatedCaption[0].strip()\n",
        "    # If token contains repeated words around ' of a ' or ' of an ', try again with different settings\n",
        "    regexPatterns = [\n",
        "        r'\\b(\\w+)\\b of a \\b\\1\\b',\n",
        "        r'\\b(\\w+)\\b of an \\b\\1\\b'\n",
        "    ]\n",
        "    if any(re.search(pattern, generatedCaption) for pattern in regexPatterns):\n",
        "        print(\"Recaptioning... âš ï¸\")\n",
        "        recaptionedImages += 1\n",
        "        generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=useNucleusSampling,\n",
        "        num_beams=9,\n",
        "        max_length=maxTokenLength,\n",
        "        min_length=minTokenLength,\n",
        "        repetition_penalty=1.9,\n",
        "        length_penalty=1,\n",
        "        top_p=topP,\n",
        "        temperature=1)\n",
        "        generatedCaption = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "        generatedCaption = generatedCaption[0].strip()\n",
        "        for pattern in regexPatterns:\n",
        "            if re.search(pattern, generatedCaption):\n",
        "                generatedCaption = re.sub(pattern, lambda match: ' '.join(match.group().split()[:-3]), generatedCaption)\n",
        "    return cleanUpCaption(generatedCaption)\n",
        "\n",
        "def generateFolderNamesAsTokens(dirpath, caption, imagePath):\n",
        "    generatedCaption = caption\n",
        "    directoryPaths = os.path.dirname(os.path.relpath(imagePath, dirpath))\n",
        "    relpathParts = [part for part in directoryPaths.split(os.sep) if \"_\" not in part and part != \".\"]\n",
        "    validParts = [part for part in relpathParts if part.lower() not in caption and part.lower()]\n",
        "    if validParts:\n",
        "        if folderNamesAtStartOrEnd == 'end':\n",
        "            generatedCaption = f\"{caption}, {', '.join(validParts)}\"\n",
        "        else:\n",
        "            generatedCaption = f\"{', '.join(validParts)}, {caption}\"\n",
        "    return generatedCaption\n",
        "\n",
        "def generateExtraDescriptors(rawImage, caption):\n",
        "    generatedExtraDescriptions = ''\n",
        "    inputs = processor(images=rawImage, text=\"Describe the style in 1 word. Answer:\", return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        max_length=10,\n",
        "        min_length=1,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    style = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "    inputs = processor(images=rawImage, text=\"Describe the theme in 1 word. Answer:\", return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        max_length=10,\n",
        "        min_length=1,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    theme = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "    inputs = processor(images=rawImage, text=\"Describe the medium in 1 word. Answer:\", return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        max_length=10,\n",
        "        min_length=1,\n",
        "        repetition_penalty=repetitionPenalty,\n",
        "        length_penalty=lengthPenalty,\n",
        "        top_p=topP,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature)\n",
        "    medium = processor.batch_decode(generated_ids, skip_special_tokens=skipSpecialTokens)\n",
        "\n",
        "    combined = OrderedDict()\n",
        "    combinedLists = style + theme + medium\n",
        "    answers = [item.strip() for sublist in combinedLists for item in re.split(',|and', sublist) if item.strip()]\n",
        "\n",
        "    for answer in answers:\n",
        "        lowerWord = answer.lower().lstrip()\n",
        "\n",
        "        if '_' in lowerWord or '??' in lowerWord or '!!' in lowerWord or 'â€”' in lowerWord or '~' in lowerWord or '@' in lowerWord or '|' in lowerWord:\n",
        "            lowerWord = ''\n",
        "\n",
        "        if 'anime' in lowerWord or 'png' in lowerWord or 'download' in lowerWord:\n",
        "            lowerWord = ''\n",
        "\n",
        "        if lowerWord.startswith(('a ', 'the ', 'and ')):\n",
        "            lowerWord = lowerWord.split(' ', 1)[1]\n",
        "\n",
        "        if lowerWord.endswith(('.', ',', '!', '?')):\n",
        "            lowerWord = lowerWord[:-1]\n",
        "\n",
        "        if lowerWord.endswith(\"'\") and lowerWord.startswith(\"'\"):\n",
        "            lowerWord = lowerWord[:-1]\n",
        "            lowerWord = lowerWord[1:]\n",
        "\n",
        "        if lowerWord.endswith(\"'\") and lowerWord.startswith(\"'\"):\n",
        "            lowerWord = lowerWord[:-1]\n",
        "            lowerWord = lowerWord[1:]\n",
        "\n",
        "        if len(lowerWord) > 1:\n",
        "            combined[lowerWord] = None\n",
        "\n",
        "    uniqueCombinedArray = list(combined)\n",
        "    uniqueImageAnswers = set(answer.lower() for answer in uniqueCombinedArray)\n",
        "    filteredImageAnswers = [ans for ans in uniqueImageAnswers if not re.search(rf'\\b{re.escape(ans)}\\b', caption.lower())]\n",
        "    if filteredImageAnswers:\n",
        "        generatedExtraDescriptions = caption + ', ' + ', '.join(filteredImageAnswers)\n",
        "    else:\n",
        "        generatedExtraDescriptions = caption\n",
        "    generatedExtraDescriptions = cleanUpCaption(generatedExtraDescriptions)\n",
        "    return generatedExtraDescriptions\n",
        "\n",
        "def cleanUpCaption(caption):\n",
        "    cleanedUpCaption = caption\n",
        "\n",
        "    if cleanedUpCaption.endswith((\"'s\")):\n",
        "        cleanedUpCaption = cleanedUpCaption[:-2]\n",
        "        \n",
        "    if cleanedUpCaption.endswith((\".\")):\n",
        "        cleanedUpCaption = cleanedUpCaption[:-1]\n",
        "    \n",
        "    # if \"there is \" in cleanedUpCaption:\n",
        "    #     # Do something\n",
        "    \n",
        "    if \" - \" in cleanedUpCaption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"t - shirt\", \"t-shirt\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\" - man\", \"-man\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\" - men\", \"-men\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"t - rex\", \"t-rex\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"sci - fi\", \"sci-fi\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"x - files\", \"x-files\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\" - stock image\", \"\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"bb - 8\", \"bb-8\")\n",
        "        \n",
        "    \n",
        "    cleanedUpCaption = cleanedUpCaption.replace('spider man', 'spiderman')\n",
        "    cleanedUpCaption = cleanedUpCaption.replace('spider-man', 'spiderman')\n",
        "    cleanedUpCaption = cleanedUpCaption.replace('pokÃ©mon', 'pokemon')\n",
        "    cleanedUpCaption = cleanedUpCaption.replace(\"http\", \"\").replace(\"www\", \"\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace(\"/\", \"\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace('\"', \"\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace('_', \"\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace(\"t.v.\", \"television\").replace(\"t.v\", \"television\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace(\" & \", \"&\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace(\"black & white\", \"black and white\")\n",
        "    cleanedUpCaption = cleanedUpCaption.replace('laying', 'lying')\n",
        "\n",
        "    if cleanedUpCaption.count(' - ') >= 3:\n",
        "        split_text = cleanedUpCaption.split('-', 1)\n",
        "        cleanedUpCaption = split_text[0]\n",
        "\n",
        "    if \"blanka\" in cleanedUpCaption:\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"hulk\", \"blanka\")\n",
        "        cleanedUpCaption = cleanedUpCaption.replace(\"green hair\", \"orange hair\")\n",
        "    return cleanedUpCaption.strip()\n",
        "\n",
        "def loadDataset(dirpath):\n",
        "    dataset = []\n",
        "    datasetRemaining = []\n",
        "    image_extensions = (\".jpg\", \".png\", \".jpeg\", \".webp\", \".gif\")\n",
        "    for root, dirs, files in os.walk(dirpath):\n",
        "        imageFiles = [f for f in files if f.lower().endswith(image_extensions)]\n",
        "        txtFiles = {f[:-4] for f in files if f.lower().endswith(\".txt\")}\n",
        "        for f in imageFiles:\n",
        "            dataset.append(os.path.join(root, f))\n",
        "            if f[:-4] not in txtFiles:\n",
        "                datasetRemaining.append(os.path.join(root, f))\n",
        "    return dataset, datasetRemaining\n",
        "\n",
        "def captionImages(dirpath):\n",
        "    global processedImages\n",
        "    global recaptionedImages\n",
        "    global totalImages\n",
        "    dataset = []\n",
        "    datasetRemaining = []\n",
        "    datasetDirectoryName = os.path.basename(dirpath)\n",
        "    textFilePath = f\"{dirpath}/{datasetDirectoryName}.txt\"\n",
        "\n",
        "    print(\"ğŸš¨ Loading dataset...\")\n",
        "    dataset, datasetRemaining = loadDataset(dirpath)\n",
        "    clear_output(wait=True)\n",
        "    print(\"ğŸš¨ Dataset loaded!\")\n",
        "    datasetSize = len(dataset)\n",
        "    totalImages = len(datasetRemaining)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for suspectIndex in range(len(datasetRemaining)):\n",
        "        processedImages += 1\n",
        "        remainingImages = totalImages - processedImages\n",
        "        caption = \"\"\n",
        "\n",
        "        imagePath = datasetRemaining[suspectIndex]\n",
        "        textFilePath = f\"{Path(imagePath).with_suffix('')}.txt\"\n",
        "\n",
        "        rawImage = Image.open(imagePath).convert('RGB')\n",
        "        if enableOverwritingExistingCaptions or not os.path.exists(textFilePath):\n",
        "            caption = generateCaption(rawImage, beams=3)\n",
        "            if addFolderNamesToCaptions:\n",
        "                caption = generateFolderNamesAsTokens(dirpath, caption, imagePath)\n",
        "            if enableExtraDescriptors:\n",
        "                caption = generateExtraDescriptors(rawImage, caption)\n",
        "\n",
        "            with open(textFilePath, 'w+') as f:\n",
        "                f.write(caption)\n",
        "                \n",
        "        elapsed_time = time.time() - start_time\n",
        "        average_time_per_image = elapsed_time / processedImages\n",
        "        estimated_remaining_time = remainingImages * average_time_per_image\n",
        "        # Format the time estimate for better readability\n",
        "        m, s = divmod(estimated_remaining_time, 60)\n",
        "        h, m = divmod(m, 60)\n",
        "\n",
        "        if remainingImages > 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"ğŸŒ Total Dataset size: {datasetSize}\")\n",
        "            if remainingImages % 4 == 0:\n",
        "                print(f\"ğŸ“« Caption: {caption}\")\n",
        "            elif remainingImages % 4 == 1:\n",
        "                print(f\"ğŸ“ª Caption: {caption}\")\n",
        "            elif remainingImages % 4 == 2:\n",
        "                print(f\"ğŸ“¬ Caption: {caption}\")\n",
        "            elif remainingImages % 4 == 3:\n",
        "                print(f\"ğŸ“­ Caption: {caption}\")\n",
        "            if remainingImages % 2 == 0:\n",
        "                print(f\"ğŸ” Processed images: {processedImages}/{totalImages} ({round((processedImages/totalImages) * 100, 2)}%)\")\n",
        "                print(f\"ğŸŒ Remaining images: {remainingImages}\")\n",
        "            else:\n",
        "                print(f\"ğŸ” Processed images: {processedImages}/{totalImages} ({round((processedImages/totalImages) * 100, 2)}%)\")\n",
        "                print(f\"ğŸŒ Remaining images: {remainingImages}\")\n",
        "            print(f\"âš¡ Recaptioned: {recaptionedImages} times\")\n",
        "            print(f\"â³ Estimated time remaining: {int(h):02d}:{int(m):02d}:{int(s):02d}\")\n",
        "            if showCurrentImage and os.path.exists(textFilePath):\n",
        "                display(rawImage.resize(( int(rawImage.width * 0.333), int(rawImage.height * 0.333))))\n",
        "        else:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"ğŸŒ Total Dataset size: {datasetSize}\")\n",
        "            print(f\"ğŸ“« Caption: {caption}\")\n",
        "            print(f\"ğŸ” Processed images: {processedImages}/{totalImages}\")\n",
        "            print(f\"ğŸŒ Remaining images: {remainingImages} âœ…\" )\n",
        "            print(f\"âš¡ Recaptioned: {recaptionedImages} times\")\n",
        "            if showCurrentImage and os.path.exists(textFilePath):\n",
        "                display(rawImage.resize(( int(rawImage.width * 0.333), int(rawImage.height * 0.333))))\n",
        "\n",
        "# Iterate through directories inside directories\n",
        "captionImages(imagesDirectory)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "blip2demo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "50c50f1f55c832fff615d4f17bdf1949c2ce06a8f6fb0f097854f91355ce9518"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
